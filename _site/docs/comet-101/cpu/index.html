<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../icon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../icon/favicon-16x16.png"><link rel="manifest" href="../icon/site.webmanifest"><link rel="mask-icon" href="../icon/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="/dev.io/assets/css/just-the-docs-default.css"><link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.indigo-blue.min.css" /> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-2709176-10"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-2709176-10', { 'anonymize_ip': true }); </script> <script type="text/javascript" src="/dev.io/assets/js/vendor/lunr.min.js"></script> <script type="text/javascript" src="/dev.io/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Compiling and Running CPU Jobss | SDSC-101-Dev</title><meta name="generator" content="Jekyll v4.1.1" /><meta property="og:title" content="Compiling and Running CPU Jobss" /><meta property="og:locale" content="en_US" /><meta name="description" content="SDSC-101 Dev branch" /><meta property="og:description" content="SDSC-101 Dev branch" /><link rel="canonical" href="http://localhost:4000/dev.io/docs/comet-101/cpu/" /><meta property="og:url" content="http://localhost:4000/dev.io/docs/comet-101/cpu/" /><meta property="og:site_name" content="SDSC-101-Dev" /> <script type="application/ld+json"> {"description":"SDSC-101 Dev branch","url":"http://localhost:4000/dev.io/docs/comet-101/cpu/","@type":"WebPage","headline":"Compiling and Running CPU Jobss","@context":"https://schema.org"}</script> <button class="btn js-toggle-dark-mode">dark scheme</button> <script> const toggleDarkMode = document.querySelector('.js-toggle-dark-mode'); jtd.addEvent(toggleDarkMode, 'click', function(){ if (jtd.getTheme() === 'dark') { jtd.setTheme('light'); toggleDarkMode.textContent = 'dark scheme'; } else { jtd.setTheme('dark'); toggleDarkMode.textContent = 'light scheme'; } }); </script><body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="http://localhost:4000/dev.io/" class="site-title lh-tight"> SDSC-101-Dev </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav role="navigation" aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/dev.io/docs/comet" class="nav-list-link">Comet 101</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/comet-101/Comet%20Overview/" class="nav-list-link">Comet Overview</a><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/comet-101/Getting%20Started%20on%20Comet/" class="nav-list-link">Getting Started on Comet</a><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/comet-101/Customizing/" class="nav-list-link">Customizing Your User Environment</a><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/comet-101/Compiling/" class="nav-list-link">Compiling & Linking</a><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/comet-101/Running%20Jobs/" class="nav-list-link">Running Jobs on Comet</a><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/dev.io/docs/comet/examples" class="nav-list-link">Hands-on Examples</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/dev.io/docs/comet-101/gpu/" class="nav-list-link">comp-and-run-cuda-jobs</a><li class="nav-list-item active"> <a href="http://localhost:4000/dev.io/docs/comet-101/cpu/" class="nav-list-link active">Compiling and Running CPU Jobss</a></ul></ul><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/dev.io/docs/expanse" class="nav-list-link">Expanse 101</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/expanse-101/Expanse%20Overview/" class="nav-list-link">Expanse Overview</a><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/expanse-101/start/" class="nav-list-link">Getting Started on Expanse</a><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/expanse-101/Customizing/" class="nav-list-link">Customizing Your User Environment</a><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/expanse-101/Compiling/" class="nav-list-link">Compiling & Linking</a><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/expanse-101/slurm/" class="nav-list-link">Using Slurm on Comet</a><li class="nav-list-item active"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/dev.io/docs/expanse/examples" class="nav-list-link">Hands-on Examples</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/dev.io/docs/expanse-101/gpu/" class="nav-list-link">comp-and-run-cuda-jobs</a><li class="nav-list-item "> <a href="http://localhost:4000/dev.io/docs/expanse-101/cpu/" class="nav-list-link">Compiling and Running CPU Jobs</a></ul></ul><li class="nav-list-item"><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/dev.io/docs/notebooks" class="nav-list-link">notebooks-101</a><ul class="nav-list "><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/notebooks-101/contactus/" class="nav-list-link">Contact Us</a><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/notebooks-101/overview/" class="nav-list-link">Jupyter Notebook Overview</a><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/notebooks-101/prerequisites/" class="nav-list-link">Software Prerequisites</a><li class="nav-list-item "><a href="http://localhost:4000/dev.io/docs/notebooks-101/aboutus/" class="nav-list-link">About the Team</a><li class="nav-list-item "><a href="#" class="nav-list-expander"><svg viewBox="0 0 24 24"><use xlink:href="#svg-arrow-right"></use></svg></a><a href="http://localhost:4000/dev.io/docs/notebooks/examples" class="nav-list-link">Example Notebooks</a><ul class="nav-list"><li class="nav-list-item "> <a href="http://localhost:4000/dev.io/docs/notebooks-101/httpConnect/" class="nav-list-link">Insecurity with direct node access</a><li class="nav-list-item "> <a href="http://localhost:4000/dev.io/docs/notebooks-101/reverseProxy/" class="nav-list-link">Security with Reverse Proxy Service</a><li class="nav-list-item "> <a href="http://localhost:4000/dev.io/docs/notebooks-101/runJupyterMethods/" class="nav-list-link">Jupyter Services on Comet</a><li class="nav-list-item "> <a href="http://localhost:4000/dev.io/docs/notebooks-101/tunneling/" class="nav-list-link">Security with SSH Tunneling</a></ul></ul></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search SDSC-101-Dev" aria-label="Search SDSC-101-Dev" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div><nav aria-label="Auxiliary" class="aux-nav"><ul class="aux-nav-list"><li class="aux-nav-list-item"> <a href="" class="site-button" > SDSC 101 on GitHub </a></ul></nav></div><div id="main-content-wrap" class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/dev.io/docs/comet">Comet 101</a><li class="breadcrumb-nav-list-item"><a href="http://localhost:4000/dev.io/docs/expanse/examples">Hands-on Examples</a><li class="breadcrumb-nav-list-item"><span>Compiling and Running CPU Jobss</span></ol></nav><div id="main-content" class="main-content" role="main"><h2 id="compiling-and-running-cpu-jobs-"> <a href="#compiling-and-running-cpu-jobs-" class="anchor-heading" aria-labelledby="compiling-and-running-cpu-jobs-"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Compiling and Running CPU Jobs: <a name="comp-and-run-cpu-jobs"></a></h2><p><b>Sections:</b></p><ul><li><a href="#hello-world-mpi">Hello World (MPI)</a><li><a href="#hello-world-omp">Hello World (OpenMP)</a><li><a href="#hybrid-mpi-omp">Running Hybrid (MPI + OpenMP) Jobs</a></ul><h3 id="hello-world-mpi"> <a href="#hello-world-mpi" class="anchor-heading" aria-labelledby="hello-world-mpi"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <a name="hello-world-mpi"></a>Hello World (MPI)</h3><p><b>Subsections:</b></p><ul><li><a href="#hello-world-mpi-source">Hello World (MPI): Source Code</a><li><a href="#hello-world-mpi-compile">Hello World (MPI): Compiling</a><li><a href="#hello-world-mpi-interactive">Hello World (MPI): Interactive Jobs</a><li><a href="#hello-world-mpi-batch-submit">Hello World (MPI): Batch Script Submission</a><li><a href="#hello-world-mpi-batch-output">Hello World (MPI): Batch Script Output</a></ul><h4 id="cpu-hello-world-source-code-hello-world-mpi-source"> <a href="#cpu-hello-world-source-code-hello-world-mpi-source" class="anchor-heading" aria-labelledby="cpu-hello-world-source-code-hello-world-mpi-source"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> CPU Hello World: Source code: &lt;#hello-world-mpi-source&gt;</h4><p>Change to the MPI examples directory (assuming you already copied the ):</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[mthomas@comet-ln3 comet101]$ cd MPI
[mthomas@comet-ln3 MPI]$ ll
[mthomas@comet-ln3:~/comet101/MPI] ll
total 498
drwxr-xr-x 4 mthomas use300      7 Apr 16 01:11 .
drwxr-xr-x 6 mthomas use300      6 Apr 15 20:10 ..
-rw-r--r-- 1 mthomas use300    336 Apr 15 15:47 hello_mpi.f90
drwxr-xr-x 2 mthomas use300      3 Apr 16 01:02 IBRUN
drwxr-xr-x 2 mthomas use300      3 Apr 16 00:57 MPIRUN_RSH
``
[mthomas@comet-ln3 OPENMP]$cat hello_mpi.f90
!  Fortran example  
program hello
include 'mpif.h'
integer rank, size, ierror, tag, status(MPI_STATUS_SIZE)

call MPI_INIT(ierror)
call MPI_COMM_SIZE(MPI_COMM_WORLD, size, ierror)
call MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierror)
print*, 'node', rank, ': Hello and Welcome to Webinar Participants!'
call MPI_FINALIZE(ierror)
end
</code></pre></div></div><p>Compile the code:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[mthomas@comet-ln3 MPI]$ mpif90 -o hello_mpi hello_mpi.f90
[mthomas@comet-ln3:~/comet101/MPI] ll
total 498
drwxr-xr-x 4 mthomas use300      7 Apr 16 01:11 .
drwxr-xr-x 6 mthomas use300      6 Apr 15 20:10 ..
-rw-r--r-- 1 mthomas use300     77 Apr 16 01:08 compile.txt
-rwxr-xr-x 1 mthomas use300 750288 Apr 16 01:11 hello_mpi
-rw-r--r-- 1 mthomas use300    336 Apr 15 15:47 hello_mpi.f90
drwxr-xr-x 2 mthomas use300      3 Apr 16 01:02 IBRUN
drwxr-xr-x 2 mthomas use300      3 Apr 16 00:57 MPIRUN_RSH
</code></pre></div></div><p>Note: The two directories that contain batch scripts needed to run the jobs using the parallel/slurm environment.</p><ul><li>First, we should verify that the user environment is correct for running the examples we will work with in this tutorial.<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[mthomas@comet-ln3 MPI]$ module list
Currently Loaded Modulefiles:
1) intel/2018.1.163    2) mvapich2_ib/2.3.2
</code></pre></div></div><li>If you have trouble with your modules, you can remove the existing environment (purge) and then reload them. After purging, the PATH variable has fewer path directories available:<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[mthomas@comet-ln3:~] module purge
[mthomas@comet-ln3:~] echo $PATH
/home/mthomas/miniconda3/bin:/home/mthomas/miniconda3/condabin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/sdsc/bin:/opt/sdsc/sbin:/opt/ibutils/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/mthomas/bin
</code></pre></div></div><li>Next, you reload the modules that you need:<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[mthomas@comet-ln3 ~]$ module load intel
[mthomas@comet-ln3 ~]$ module load mvapich2_ib
</code></pre></div></div><li>You will see that there are more binaries in the PATH:<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[mthomas@comet-ln3:~] echo $PATH
/opt/mvapich2/intel/ib/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64:/home/mthomas/miniconda3/bin:/home/mthomas/miniconda3/condabin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/sdsc/bin:/opt/sdsc/sbin:/opt/ibutils/bin:/opt/pdsh/bin:/opt/rocks/bin:/opt/rocks/sbin:/home/mthomas/bin
</code></pre></div></div></ul><p><a href="#comp-and-run-cpu-jobs">Back to CPU Jobs</a> <br /> <a href="#top">Back to Top</a></p><hr /><h4 id="hello-world-mpi-compiling-"> <a href="#hello-world-mpi-compiling-" class="anchor-heading" aria-labelledby="hello-world-mpi-compiling-"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Hello World (MPI): Compiling: <a name="hello-world-mpi-compile"></a></h4><ul><li>Compile the MPI hello world code.<li>For this, we use the command <code class="language-plaintext highlighter-rouge">mpif90</code>, which is loaded into your environment when you loaded the intel module above.<li>To see where the command is located, use the <code class="language-plaintext highlighter-rouge">which</code> command:<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[mthomas@comet-ln3 MPI]$ which mpif90
/opt/mvapich2/intel/ib/bin/mpif90
</code></pre></div></div><li>Compile the code:<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mpif90 -o hello_mpi hello_mpi.f90
</code></pre></div></div><li>Verify that the executable has been created:</ul><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[mthomas@comet-ln3:~/comet101/MPI] ll
total 498
drwxr-xr-x 4 mthomas use300      7 Apr 16 01:11 .
drwxr-xr-x 6 mthomas use300      6 Apr 15 20:10 ..
-rwxr-xr-x 1 mthomas use300 750288 Apr 16 01:11 hello_mpi
-rw-r--r-- 1 mthomas use300    336 Apr 15 15:47 hello_mpi.f90
drwxr-xr-x 2 mthomas use300      3 Apr 16 01:02 IBRUN
drwxr-xr-x 2 mthomas use300      3 Apr 16 00:57 MPIRUN_RSH
</code></pre></div></div><ul><li>In the next sections, we will see how to run parallel code using two environments:<li>Running a parallel job on an <em>Interactive</em> compute node<li>Running parallel code using the batch queue system</ul><p><a href="#comp-and-run-cpu-jobs">Back to CPU Jobs</a> <br /> <a href="#top">Back to Top</a></p><hr /><h4 id="hello-world-mpi-interactive-jobs-"> <a href="#hello-world-mpi-interactive-jobs-" class="anchor-heading" aria-labelledby="hello-world-mpi-interactive-jobs-"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Hello World (MPI): Interactive Jobs: <a name="hello-world-mpi-interactive"></a></h4><ul><li>To run MPI (or other executables) from the command line, you need to use the “Interactive” nodes.<li>To launch the nodes (to get allocated a set of nodes), use the <code class="language-plaintext highlighter-rouge">srun</code> command. This example will request one node, all 24 cores, in the debug partition for 30 minutes:<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[mthomas@comet-ln3:~/comet101/MPI] date
Thu Apr 16 01:21:48 PDT 2020
[mthomas@comet-ln3:~/comet101/MPI] srun --pty --nodes=1 --ntasks-per-node=24 -p debug -t 00:30:00 --wait 0 /bin/bash
[mthomas@comet-14-01:~/comet101/MPI] date
Thu Apr 16 01:22:42 PDT 2020
[mthomas@comet-14-01:~/comet101/MPI] hostname
comet-14-01.sdsc.edu
</code></pre></div></div><li>Note:<li>You will know when you have an interactive node because the srun command will return and you will be on a different host.<li>Note: If the cluster is very busy, it may take some time to obtain the nodes.<li>Once you have the interactive session, your MPI code will be allowed to execute on the command line.<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[mthomas@comet-14-01 MPI]$ mpirun -np 4 ./hello_mpi
node           0 : Hello and Welcome to Webinar Participants!
node           1 : Hello and Welcome to Webinar Participants!
node           2 : Hello and Welcome to Webinar Participants!
node           3 : Hello and Welcome to Webinar Participants!
[mthomas@comet-14-01 MPI]$
</code></pre></div></div></ul><p>When you are done testing code, exit the Interactive session.</p><p><a href="#comp-and-run-cpu-jobs">Back to CPU Jobs</a> <br /> <a href="#top">Back to Top</a></p><hr /><h4 id="hello-world-mpi-batch-script-submission--"> <a href="#hello-world-mpi-batch-script-submission--" class="anchor-heading" aria-labelledby="hello-world-mpi-batch-script-submission--"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Hello World (MPI): Batch Script Submission: <a name="hello-world-mpi-batch-submit"></a></h4><p>To submit jobs to the Slurm queuing system, you need to create a slurm batch job script and submit it to the queuing system.</p><ul><li>Change directories to the IBRUN directory using the <code class="language-plaintext highlighter-rouge">hellompi-slurm.sb</code> batch script: ``` [mthomas@comet-ln3 MPI]$ cd IBRUN/ [mthomas@comet-ln3 IBRUN]$ cat hellompi-slurm.sb #!/bin/bash #SBATCH –job-name=”hellompi” #SBATCH –output=”hellompi.%j.%N.out” #SBATCH –partition=compute #SBATCH –nodes=2 #SBATCH –ntasks-per-node=24 #SBATCH –export=ALL #SBATCH -t 00:30:00</ul><h1 id="load-the-user-environment"> <a href="#load-the-user-environment" class="anchor-heading" aria-labelledby="load-the-user-environment"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> load the user environment</h1><p>source /etc/profile.d/modules.sh module purge module load intel module load mvapich2_ib</p><p>#This job runs with 2 nodes, 24 cores per node for a total of 48 cores. #ibrun in verbose mode will give binding detail</p><p>ibrun -v ../hello_mpi</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* to run the job, use the command below:
</code></pre></div></div><p>[mthomas@comet-ln3 IBRUN]$ sbatch hellompi.sb Submitted batch job 32662205</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* In some cases, you may have access to a reservation queue, use the command below:
</code></pre></div></div><p>sbatch –res=SI2018DAY1 hellompi-slurm.sb</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
[Back to CPU Jobs](#comp-and-run-cpu-jobs) &lt;br&gt;
[Back to Top](#top)
&lt;hr&gt;

#### Hello World (MPI): Batch Script Output: &lt;a name="hello-world-mpi-batch-output"&gt;&lt;/a&gt;

* Check job status using the `squeue` command.
</code></pre></div></div><p>[mthomas@comet-ln3 IBRUN]$ sbatch hellompi-slurm.sb; squeue -u username Submitted batch job 18345138 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 32662205 compute hellompi username PD 0:00 2 (None) ….</p><p>[mthomas@comet-ln3 IBRUN]$ squeue -u username JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 32662205 compute hellompi username R 0:07 2 comet-21-[47,57] [mthomas@comet-ln3 IBRUN]$ squeue -u username JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 32662205 compute hellompi username CG 0:08 2 comet-21-[47,57]</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* Note: You will see the `ST` column information change when the job status changes: new jobs go into  `SP` (pending); after some time it moves to  `R` (running): when completed, the state changes to `CG` (completed)
* the JOBID is the job identifer and can be used to track or cancel the job. It is also used as part of the output file name.

* Look at the directory for and output file with the job id as part of the name:
</code></pre></div></div><p>[mthomas@comet-ln3 IBRUN]$ total 48 drwxr-xr-x 2 mthomas use300 4 Apr 16 01:31 . drwxr-xr-x 4 mthomas use300 7 Apr 16 01:11 .. -rw-r–r– 1 mthomas use300 2873 Apr 16 01:31 hellompi.32662205.comet-20-03.out -rw-r–r– 1 mthomas use300 341 Apr 16 01:30 hellompi-slurm.sb</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
* To see the contents of the output file, use the `cat` command:
</code></pre></div></div><p>[mthomas@comet-ln3 IBRUN]$ cat hellompi.32662205.comet-20-03.out IBRUN: Command is ../hello_mpi IBRUN: Command is /home/username/comet-examples/comet101/MPI/hello_mpi IBRUN: no hostfile mod needed IBRUN: Nodefile is /tmp/0p4Nbx12u1</p><p>IBRUN: MPI binding policy: compact/core for 1 threads per rank (12 cores per socket) IBRUN: Adding MV2_USE_OLD_BCAST=1 to the environment IBRUN: Adding MV2_CPU_BINDING_LEVEL=core to the environment IBRUN: Adding MV2_ENABLE_AFFINITY=1 to the environment IBRUN: Adding MV2_DEFAULT_TIME_OUT=23 to the environment IBRUN: Adding MV2_CPU_BINDING_POLICY=bunch to the environment IBRUN: Adding MV2_USE_HUGEPAGES=0 to the environment IBRUN: Adding MV2_HOMOGENEOUS_CLUSTER=0 to the environment IBRUN: Adding MV2_USE_UD_HYBRID=0 to the environment IBRUN: Added 8 new environment variables to the execution environment IBRUN: Command string is [mpirun_rsh -np 48 -hostfile /tmp/0p4Nbx12u1 -export-all /home/username/comet-examples/comet101/MPI/hello_mpi] node 18 : Hello and Welcome to Webinar Participants! node 17 : Hello and Welcome to Webinar Participants! node 20 : Hello and Welcome to Webinar Participants! node 21 : Hello and Welcome to Webinar Participants! node 22 : Hello and Welcome to Webinar Participants! node 5 : Hello and Welcome to Webinar Participants! node 3 : Hello and Welcome to Webinar Participants! node 6 : Hello and Welcome to Webinar Participants! node 16 : Hello and Welcome to Webinar Participants! node 19 : Hello and Welcome to Webinar Participants! node 14 : Hello and Welcome to Webinar Participants! node 10 : Hello and Welcome to Webinar Participants! node 13 : Hello and Welcome to Webinar Participants! node 15 : Hello and Welcome to Webinar Participants! node 9 : Hello and Welcome to Webinar Participants! node 12 : Hello and Welcome to Webinar Participants! node 4 : Hello and Welcome to Webinar Participants! node 23 : Hello and Welcome to Webinar Participants! node 7 : Hello and Welcome to Webinar Participants! node 11 : Hello and Welcome to Webinar Participants! node 8 : Hello and Welcome to Webinar Participants! node 1 : Hello and Welcome to Webinar Participants! node 2 : Hello and Welcome to Webinar Participants! node 0 : Hello and Welcome to Webinar Participants! node 39 : Hello and Welcome to Webinar Participants! node 38 : Hello and Welcome to Webinar Participants! node 47 : Hello and Welcome to Webinar Participants! node 45 : Hello and Welcome to Webinar Participants! node 42 : Hello and Welcome to Webinar Participants! node 35 : Hello and Welcome to Webinar Participants! node 28 : Hello and Welcome to Webinar Participants! node 32 : Hello and Welcome to Webinar Participants! node 40 : Hello and Welcome to Webinar Participants! node 44 : Hello and Welcome to Webinar Participants! node 41 : Hello and Welcome to Webinar Participants! node 30 : Hello and Welcome to Webinar Participants! node 31 : Hello and Welcome to Webinar Participants! node 29 : Hello and Welcome to Webinar Participants! node 37 : Hello and Welcome to Webinar Participants! node 43 : Hello and Welcome to Webinar Participants! node 46 : Hello and Welcome to Webinar Participants! node 34 : Hello and Welcome to Webinar Participants! node 26 : Hello and Welcome to Webinar Participants! node 24 : Hello and Welcome to Webinar Participants! node 27 : Hello and Welcome to Webinar Participants! node 25 : Hello and Welcome to Webinar Participants! node 33 : Hello and Welcome to Webinar Participants! node 36 : Hello and Welcome to Webinar Participants! IBRUN: Job ended with value 0 [mthomas@comet-ln3 IBRUN]$</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* Note the order in which the output was written into the output file. There is an entry for each of the 48 cores (2 nodes, 24 cores/node), but the output is not ordered. This is typical because the time for each core to start and finish its work is asynchronous.

[Back to CPU Jobs](#comp-and-run-cpu-jobs) &lt;br&gt;
[Back to Top](#top)
&lt;hr&gt;

### Hello World (OpenMP): &lt;a name="hello-world-omp"&gt;&lt;/a&gt;
&lt;b&gt;Subsections:&lt;/b&gt;
* [Hello World (OpenMP): Source Code](#hello-world-omp-source)
* [Hello World (OpenMP): Compiling](#hello-world-omp-compile)
* [Hello World (OpenMP): Batch Script Submission](#hello-world-omp-batch-submit)
* [Hello World (OpenMP): Batch Script Output](#hello-world-omp-batch-output)


#### Hello World (OpenMP): Source Code &lt;a name="hello-world-omp-source"&gt;&lt;/a&gt;

Change to the OPENMP examples directory:
</code></pre></div></div><p>[mthomas@comet-ln3 comet101]$ cd OPENMP/ [mthomas@comet-ln3 OPENMP]$ ls -al total 479 drwxr-xr-x 2 username use300 6 Aug 5 22:19 . drwxr-xr-x 16 username use300 16 Aug 5 19:02 .. -rwxr-xr-x 1 username use300 728112 Aug 5 19:02 hello_openmp -rw-r–r– 1 username use300 267 Aug 5 22:19 hello_openmp.f90 -rw-r–r– 1 username use300 310 Aug 5 19:02 openmp-slurm.sb -rw-r–r– 1 username use300 347 Aug 5 19:02 openmp-slurm-shared.sb</p><p>[mthomas@comet-ln3 OPENMP]$ cat hello_openmp.f90 PROGRAM OMPHELLO INTEGER TNUMBER INTEGER OMP_GET_THREAD_NUM</p><p>!$OMP PARALLEL DEFAULT(PRIVATE) TNUMBER = OMP_GET_THREAD_NUM() PRINT *, ‘Hello from Thread Number[‘,TNUMBER,’] and Welcome Webinar!’ !$OMP END PARALLEL</p><p>STOP END</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Back to CPU Jobs](#comp-and-run-cpu-jobs) &lt;br&gt;
[Back to Top](#top)
&lt;hr&gt;

#### Hello World (OpenMP): Compiling:  &lt;a name="hello-world-omp-compile"&gt;&lt;/a&gt;

Note that there is already a compiled version of the `hello_openmp.f90` code. You can save or delete this version.

* In this example, we compile the source code using the `ifort` command, and verify that it was created:
</code></pre></div></div><p>[mthomas@comet-ln3 OPENMP]$ ifort -o hello_openmp -qopenmp hello_openmp.f90 [mthomas@comet-ln3 OPENMP]$ ls -al [mthomas@comet-ln3:~/comet101/OPENMP] ll total 77 drwxr-xr-x 2 mthomas use300 7 Apr 16 00:35 . drwxr-xr-x 6 mthomas use300 6 Apr 15 20:10 .. -rwxr-xr-x 1 mthomas use300 816952 Apr 16 00:35 hello_openmp -rw-r–r– 1 mthomas use300 267 Apr 15 15:47 hello_openmp_2.f90 -rw-r–r– 1 mthomas use300 267 Apr 15 15:47 hello_openmp.f90 -rw-r–r– 1 mthomas use300 311 Apr 15 15:47 openmp-slurm.sb -rw-r–r– 1 mthomas use300 347 Apr 15 15:47 openmp-slurm-shared.sb</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* Note that if you try to run OpenMP code from the command line, in the current environment, the code will run (because it is based on Pthreads, which exist on the node):
</code></pre></div></div><p>[mthomas@comet-ln2 OPENMP]$ ./hello_openmp Hello from Thread Number[ 8 ] and Welcome HPC Trainees! Hello from Thread Number[ 3 ] and Welcome HPC Trainees! Hello from Thread Number[ 16 ] and Welcome HPC Trainees! Hello from Thread Number[ 12 ] and Welcome HPC Trainees! Hello from Thread Number[ 9 ] and Welcome HPC Trainees! Hello from Thread Number[ 5 ] and Welcome HPC Trainees! Hello from Thread Number[ 4 ] and Welcome HPC Trainees! Hello from Thread Number[ 14 ] and Welcome HPC Trainees! Hello from Thread Number[ 7 ] and Welcome HPC Trainees! Hello from Thread Number[ 11 ] and Welcome HPC Trainees! Hello from Thread Number[ 13 ] and Welcome HPC Trainees! Hello from Thread Number[ 6 ] and Welcome HPC Trainees! Hello from Thread Number[ 10 ] and Welcome HPC Trainees! Hello from Thread Number[ 19 ] and Welcome HPC Trainees! Hello from Thread Number[ 15 ] and Welcome HPC Trainees! Hello from Thread Number[ 2 ] and Welcome HPC Trainees! Hello from Thread Number[ 18 ] and Welcome HPC Trainees! Hello from Thread Number[ 17 ] and Welcome HPC Trainees! Hello from Thread Number[ 23 ] and Welcome HPC Trainees! Hello from Thread Number[ 20 ] and Welcome HPC Trainees! Hello from Thread Number[ 22 ] and Welcome HPC Trainees! Hello from Thread Number[ 1 ] and Welcome HPC Trainees! Hello from Thread Number[ 0 ] and Welcome HPC Trainees! Hello from Thread Number[ 21 ] and Welcome HPC Trainees!</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* In the example below, we used the OpenMP feature to set the number of threads from the command line.

</code></pre></div></div><p>[mthomas@comet-ln3 OPENMP]$ export OMP_NUM_THREADS=4; ./hello_openmp Hello from Thread Number[ 0 ] and Welcome HPC Trainees! Hello from Thread Number[ 1 ] and Welcome HPC Trainees! Hello from Thread Number[ 2 ] and Welcome HPC Trainees! Hello from Thread Number[ 3 ] and Welcome HPC Trainees!</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
[Back to CPU Jobs](#comp-and-run-cpu-jobs) &lt;br&gt;
[Back to Top](#top)
&lt;hr&gt;

#### &lt;a name="hello-world-omp-batch-submit"&gt;&lt;/a&gt;Hello World (OpenMP): Batch Script Submission
The submit script is openmp-slurm.sb:

</code></pre></div></div><p>[mthomas@comet-ln2 OPENMP]$ cat openmp-slurm.sb #!/bin/bash #SBATCH –job-name=”hello_openmp” #SBATCH –output=”hello_openmp.%j.%N.out” #SBATCH –partition=compute #SBATCH –nodes=1 #SBATCH –ntasks-per-node=24 #SBATCH –export=ALL #SBATCH -t 01:30:00</p><h1 id="define-the-user-environment"> <a href="#define-the-user-environment" class="anchor-heading" aria-labelledby="define-the-user-environment"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Define the user environment</h1><p>source /etc/profile.d/modules.sh module purge module load intel module load mvapich2_ib</p><p>#SET the number of openmp threads export OMP_NUM_THREADS=24</p><p>#Run the job using mpirun_rsh ./hello_openmp</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* to submit use the sbatch command:
</code></pre></div></div><p>[mthomas@comet-ln2 OPENMP]$ sbatch openmp-slurm.sb Submitted batch job 32661678 [mthomas@comet-ln2 OPENMP]$ squeue -u username JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 32661678 compute hello_op mthomas PD 0:00 1 (Priority) …</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
[Back to CPU Jobs](#comp-and-run-cpu-jobs) &lt;br&gt;
[Back to Top](#top)
&lt;hr&gt;

#### Hello World (OpenMP): Batch Script Output:  &lt;a name="hello-world-omp-batch-output"&gt;&lt;/a&gt;

* Once the job is finished:
</code></pre></div></div><p>[mthomas@comet-ln2 OPENMP] cat hello_openmp.32661678.comet-07-47.out Hello from Thread Number[ 5 ] and Welcome HPC Trainees! Hello from Thread Number[ 7 ] and Welcome HPC Trainees! Hello from Thread Number[ 16 ] and Welcome HPC Trainees! Hello from Thread Number[ 9 ] and Welcome HPC Trainees! Hello from Thread Number[ 18 ] and Welcome HPC Trainees! Hello from Thread Number[ 12 ] and Welcome HPC Trainees! Hello from Thread Number[ 10 ] and Welcome HPC Trainees! Hello from Thread Number[ 0 ] and Welcome HPC Trainees! Hello from Thread Number[ 14 ] and Welcome HPC Trainees! Hello from Thread Number[ 4 ] and Welcome HPC Trainees! Hello from Thread Number[ 3 ] and Welcome HPC Trainees! Hello from Thread Number[ 11 ] and Welcome HPC Trainees! Hello from Thread Number[ 19 ] and Welcome HPC Trainees! Hello from Thread Number[ 22 ] and Welcome HPC Trainees! Hello from Thread Number[ 15 ] and Welcome HPC Trainees! Hello from Thread Number[ 2 ] and Welcome HPC Trainees! Hello from Thread Number[ 6 ] and Welcome HPC Trainees! Hello from Thread Number[ 1 ] and Welcome HPC Trainees! Hello from Thread Number[ 21 ] and Welcome HPC Trainees! Hello from Thread Number[ 20 ] and Welcome HPC Trainees! Hello from Thread Number[ 17 ] and Welcome HPC Trainees! Hello from Thread Number[ 23 ] and Welcome HPC Trainees! Hello from Thread Number[ 13 ] and Welcome HPC Trainees! Hello from Thread Number[ 8 ] and Welcome HPC Trainees!</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
[Back to CPU Jobs](#comp-and-run-cpu-jobs) &lt;br&gt;
[Back to Top](#top)
&lt;hr&gt;

### Hybrid (MPI + OpenMP) Jobs: &lt;a name="hybrid-mpi-omp"&gt;&lt;/a&gt;
&lt;b&gt;Subsections:&lt;/b&gt;
* [Hybrid (MPI + OpenMP): Source Code](#hybrid-mpi-omp-source)
* [Hybrid (MPI + OpenMP): Compiling](#hybrid-mpi-omp-compile)
* [Hybrid (MPI + OpenMP): Batch Script Submission](#hybrid-mpi-omp-batch-submit)
* [Hybrid (MPI + OpenMP): Batch Script Output](#hybrid-mpi-omp-batch-output)


### Hybrid (MPI + OpenMP) Source Code: &lt;a name="hybrid-mpi-omp-source"&gt;&lt;/a&gt;
#Several HPC codes use a hybrid MPI, OpenMP approach.
* `ibrun` wrapper developed to handle such hybrid use cases. Automatically senses the MPI build (mvapich2, openmpi) and binds tasks correctly.
* `ibrun -help` gives detailed usage info.
* hello_hybrid.c is a sample code, and hello_hybrid.cmd shows “ibrun” usage.
* Change to the HYBRID examples directory:

</code></pre></div></div><p>[mthomas@comet-ln2 comet101]$ cd HYBRID/ [mthomas@comet-ln2 HYBRID]$ ll total 94 drwxr-xr-x 2 username use300 5 Aug 5 19:02 . drwxr-xr-x 16 username use300 16 Aug 5 19:02 .. -rwxr-xr-x 1 username use300 103032 Aug 5 19:02 hello_hybrid -rw-r–r– 1 username use300 636 Aug 5 19:02 hello_hybrid.c -rw-r–r– 1 username use300 390 Aug 5 19:02 hybrid-slurm.sb</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* Look at the contents of the `hello_hybrid.c` file
</code></pre></div></div><p>[mthomas@comet-ln2 HYBRID]$ cat hello_hybrid.c #include <stdio.h> #include "mpi.h" #include <omp.h></omp.h></stdio.h></p><p>int main(int argc, char *argv[]) { int numprocs, rank, namelen; char processor_name[MPI_MAX_PROCESSOR_NAME]; int iam = 0, np = 1;</p><p>MPI_Init(&amp;argc, &amp;argv); MPI_Comm_size(MPI_COMM_WORLD, &amp;numprocs); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); MPI_Get_processor_name(processor_name, &amp;namelen);</p><p>#pragma omp parallel default(shared) private(iam, np) { np = omp_get_num_threads(); iam = omp_get_thread_num(); printf(“Hello Webinar participants from thread %d out of %d from process %d out of %d on %s\n”, iam, np, rank, numprocs, processor_name); }</p><p>MPI_Finalize(); }</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
[Back to CPU Jobs](#comp-and-run-cpu-jobs) &lt;br&gt;
[Back to Top](#top)
&lt;hr&gt;

#### Hybrid (MPI + OpenMP): Compiling:  &lt;a name="hybrid-mpi-omp-compile"&gt;&lt;/a&gt;
* To compile the hybrid MPI + OpenMPI code, we need to refer to the table of compilers listed above (and listed in the user guide).
* We will use the command `mpicx -openmp`
</code></pre></div></div><p>[mthomas@comet-ln2 HYBRID]$ mpicc -openmp -o hello_hybrid hello_hybrid.c [mthomas@comet-ln2 HYBRID]$ ll total 39 drwxr-xr-x 2 username use300 5 Aug 6 00:12 . drwxr-xr-x 16 username use300 16 Aug 5 19:02 .. -rwxr-xr-x 1 username use300 103032 Aug 6 00:12 hello_hybrid -rw-r–r– 1 username use300 636 Aug 5 19:02 hello_hybrid.c -rw-r–r– 1 username use300 390 Aug 5 19:02 hybrid-slurm.sb</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
[Back to CPU Jobs](#comp-and-run-cpu-jobs) &lt;br&gt;
[Back to Top](#top)
&lt;hr&gt;


#### Hybrid (MPI + OpenMP): Batch Script Submission:  &lt;a name="hybrid-mpi-omp-batch-submit"&gt;&lt;/a&gt;
* To submit the hybrid code, we still use the `ibrun` command.
* In this example, we set the number of threads explicitly.
</code></pre></div></div><p>[mthomas@comet-ln2 HYBRID]$ cat hybrid-slurm.sb #!/bin/bash #SBATCH –job-name=”hellohybrid” #SBATCH –output=”hellohybrid.%j.%N.out” #SBATCH –partition=compute #SBATCH –nodes=2 #SBATCH –ntasks-per-node=24 #SBATCH –export=ALL #SBATCH -t 01:30:00</p><h1 id="define-the-user-environment-1"> <a href="#define-the-user-environment-1" class="anchor-heading" aria-labelledby="define-the-user-environment-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Define the user environment</h1><p>source /etc/profile.d/modules.sh module purge module load intel module load mvapich2_ib</p><p>#This job runs with 2 nodes, 24 cores per node for a total of 48 cores.</p><h1 id="we-use-8-mpi-tasks-and-6-openmp-threads-per-mpi-task"> <a href="#we-use-8-mpi-tasks-and-6-openmp-threads-per-mpi-task" class="anchor-heading" aria-labelledby="we-use-8-mpi-tasks-and-6-openmp-threads-per-mpi-task"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> We use 8 MPI tasks and 6 OpenMP threads per MPI task</h1><p>export OMP_NUM_THREADS=6 ibrun –npernode 4 ./hello_hybrid</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* Submit the job to the Slurm queue, and check the job status
</code></pre></div></div><p>[mthomas@comet-ln2 HYBRID]$ sbatch hybrid-slurm.sb Submitted batch job 18347079 [mthomas@comet-ln2 HYBRID]$ squeue -u username JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 18347079 compute hellohyb username R 0:04 2 comet-01-[01,04] [mthomas@comet-ln2 HYBRID]$ ll</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
[Back to CPU Jobs](#comp-and-run-cpu-jobs) &lt;br&gt;
[Back to Top](#top)
&lt;hr&gt;


#### Hybrid (MPI + OpenMP): Batch Script Output: &lt;a name="hybrid-mpi-omp-batch-output"&gt;&lt;/a&gt;

</code></pre></div></div><p>[mthomas@comet-ln2 HYBRID]$ ll total 122 drwxr-xr-x 2 username use300 6 Aug 6 00:12 . drwxr-xr-x 16 username use300 16 Aug 5 19:02 .. -rwxr-xr-x 1 username use300 103032 Aug 6 00:12 hello_hybrid -rw-r–r– 1 username use300 3696 Aug 6 00:12 hellohybrid.18347079.comet-01-01.out -rw-r–r– 1 username use300 636 Aug 5 19:02 hello_hybrid.c -rw-r–r– 1 username use300 390 Aug 5 19:02 hybrid-slurm.sb [mthomas@comet-ln2 HYBRID]$ cat hellohybrid.18347079.comet-01-01.out Hello from thread 4 out of 6 from process 3 out of 8 on comet-01-01.sdsc.edu Hello from thread 3 out of 6 from process 2 out of 8 on comet-01-01.sdsc.edu Hello from thread 0 out of 6 from process 1 out of 8 on comet-01-01.sdsc.edu Hello from thread 2 out of 6 from process 2 out of 8 on comet-01-01.sdsc.edu Hello from thread 1 out of 6 from process 3 out of 8 on comet-01-01.sdsc.edu Hello from thread 2 out of 6 from process 1 out of 8 on comet-01-01.sdsc.edu Hello from thread 4 out of 6 from process 2 out of 8 on comet-01-01.sdsc.edu Hello from thread 0 out of 6 from process 3 out of 8 on comet-01-01.sdsc.edu Hello from thread 3 out of 6 from process 1 out of 8 on comet-01-01.sdsc.edu Hello from thread 5 out of 6 from process 2 out of 8 on comet-01-01.sdsc.edu Hello from thread 3 out of 6 from process 3 out of 8 on comet-01-01.sdsc.edu Hello from thread 4 out of 6 from process 1 out of 8 on comet-01-01.sdsc.edu Hello from thread 0 out of 6 from process 2 out of 8 on comet-01-01.sdsc.edu Hello from thread 2 out of 6 from process 3 out of 8 on comet-01-01.sdsc.edu Hello from thread 5 out of 6 from process 1 out of 8 on comet-01-01.sdsc.edu Hello from thread 5 out of 6 from process 3 out of 8 on comet-01-01.sdsc.edu Hello from thread 3 out of 6 from process 0 out of 8 on comet-01-01.sdsc.edu Hello from thread 2 out of 6 from process 0 out of 8 on comet-01-01.sdsc.edu Hello from thread 0 out of 6 from process 0 out of 8 on comet-01-01.sdsc.edu Hello from thread 4 out of 6 from process 0 out of 8 on comet-01-01.sdsc.edu Hello from thread 5 out of 6 from process 0 out of 8 on comet-01-01.sdsc.edu Hello from thread 1 out of 6 from process 2 out of 8 on comet-01-01.sdsc.edu Hello from thread 1 out of 6 from process 1 out of 8 on comet-01-01.sdsc.edu Hello from thread 1 out of 6 from process 0 out of 8 on comet-01-01.sdsc.edu Hello from thread 0 out of 6 from process 7 out of 8 on comet-01-04.sdsc.edu Hello from thread 0 out of 6 from process 6 out of 8 on comet-01-04.sdsc.edu Hello from thread 2 out of 6 from process 7 out of 8 on comet-01-04.sdsc.edu Hello from thread 2 out of 6 from process 6 out of 8 on comet-01-04.sdsc.edu Hello from thread 3 out of 6 from process 7 out of 8 on comet-01-04.sdsc.edu Hello from thread 5 out of 6 from process 6 out of 8 on comet-01-04.sdsc.edu Hello from thread 4 out of 6 from process 6 out of 8 on comet-01-04.sdsc.edu Hello from thread 1 out of 6 from process 7 out of 8 on comet-01-04.sdsc.edu Hello from thread 4 out of 6 from process 7 out of 8 on comet-01-04.sdsc.edu Hello from thread 1 out of 6 from process 6 out of 8 on comet-01-04.sdsc.edu Hello from thread 0 out of 6 from process 4 out of 8 on comet-01-04.sdsc.edu Hello from thread 5 out of 6 from process 4 out of 8 on comet-01-04.sdsc.edu Hello from thread 2 out of 6 from process 4 out of 8 on comet-01-04.sdsc.edu Hello from thread 1 out of 6 from process 4 out of 8 on comet-01-04.sdsc.edu Hello from thread 3 out of 6 from process 4 out of 8 on comet-01-04.sdsc.edu Hello from thread 4 out of 6 from process 4 out of 8 on comet-01-04.sdsc.edu Hello from thread 0 out of 6 from process 5 out of 8 on comet-01-04.sdsc.edu Hello from thread 1 out of 6 from process 5 out of 8 on comet-01-04.sdsc.edu Hello from thread 4 out of 6 from process 5 out of 8 on comet-01-04.sdsc.edu Hello from thread 2 out of 6 from process 5 out of 8 on comet-01-04.sdsc.edu Hello from thread 5 out of 6 from process 5 out of 8 on comet-01-04.sdsc.edu Hello from thread 3 out of 6 from process 6 out of 8 on comet-01-04.sdsc.edu Hello from thread 5 out of 6 from process 7 out of 8 on comet-01-04.sdsc.edu Hello from thread 3 out of 6 from process 5 out of 8 on comet-01-04.sdsc.edu [mthomas@comet-ln2 HYBRID]$ ```</p><p><a href="#comp-and-run-cpu-jobs">Back to CPU Jobs</a> <br /> <a href="#top">Back to Top</a></p><hr /><hr><footer><p><a href="#top" id="back-to-top">Back to top</a></p><p class="text-small text-grey-dk-000 mb-0">Copyright &copy; Cyber Infrastructure Machine Learning MIT license.</a></p><div class="d-flex mt-2"><p class="text-small text-grey-dk-000 mb-0"> <a href="https://github.com/Lzy17/expanse101-Jekyll/tree/main/docs/comet-101/cpu.md" id="edit-this-page">Edit this page on GitHub</a></p></div></footer></div></div><div class="search-overlay"></div></div>
